version: '3.8'

services:
  # Frontend React Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
      - REACT_APP_WS_URL=ws://localhost:8000
    volumes:
      - ./frontend/src:/app/src
      - ./frontend/public:/app/public
    depends_on:
      - backend
    networks:
      - musicai-network
    restart: unless-stopped

  # Backend FastAPI Application
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - TEMP_DIR=/app/temp_files
      - MODEL_CACHE_DIR=/app/models
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4
    volumes:
      - ./temp_files:/app/temp_files
      - ./models:/app/models
      - model-cache:/root/.cache/torch
    depends_on:
      - redis
    networks:
      - musicai-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # AI Worker for Background Processing
  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.worker
    environment:
      - REDIS_URL=redis://redis:6379
      - TEMP_DIR=/app/temp_files
      - MODEL_CACHE_DIR=/app/models
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4
      - WORKER_ID=worker-1
    volumes:
      - ./temp_files:/app/temp_files
      - ./models:/app/models
      - model-cache:/root/.cache/torch
    depends_on:
      - redis
    networks:
      - musicai-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Redis for Caching and Job Queue
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - musicai-network
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru

  # Nginx Reverse Proxy (Optional)
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    networks:
      - musicai-network
    restart: unless-stopped

  # Monitoring with Prometheus (Optional)
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - musicai-network
    restart: unless-stopped
    profiles:
      - monitoring

  # Grafana for Metrics Visualization (Optional)
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    networks:
      - musicai-network
    restart: unless-stopped
    profiles:
      - monitoring

volumes:
  redis-data:
    driver: local
  model-cache:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

networks:
  musicai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16